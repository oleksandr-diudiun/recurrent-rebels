{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53276896-3a7c-4e32-a36f-69a5d22f0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43151154-c9f4-4485-a66f-9332d029ec93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "150\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(iris.target))\n",
    "print(iris.target.size)\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0163dd79-ef72-47d4-8f5d-f7b8756cc7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "'''\n",
    "# since this is a bunch, create a dataframe\n",
    "iris_df=pd.DataFrame(iris.data)\n",
    "iris_df['class']=iris.target\n",
    "iris_df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "iris_df.dropna(how=\"all\", inplace=True) # remove any empty lines\n",
    "\n",
    "#view the iris dataframe\n",
    "print(iris_df)\n",
    "'''\n",
    "\n",
    "# Convert to DataFrame for convenience\n",
    "data = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = data.iloc[:, :-1].values\n",
    "y_val = y = data.iloc[:, -1].values\n",
    "y = y_val.reshape(-1, 1) # Reshape for one-hot encoding\n",
    "#print(type(y))\n",
    "#print(y_val.shape, y.shape)\n",
    "#print(y_val[:10])\n",
    "#print(y[:10])\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "#print(X[:10])\n",
    "print(y[:10])\n",
    "#print(y[0].shape)\n",
    "#print(X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226efbe-393b-4ff2-90e8-e3fef01a973e",
   "metadata": {},
   "source": [
    "### Step 3 - Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "825a2b46-8cb0-4a7e-91d0-dda002234456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a36541-268d-47d8-8609-a40007a3fe8b",
   "metadata": {},
   "source": [
    "### Step 4 - Define Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1bac74c8-5021-481f-92e7-a6636da5fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1] # 4 features\n",
    "hidden_size = 5\n",
    "output_size = y_train.shape[1] # 3 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71ac53-6803-4996-8ccf-f658c691f7a0",
   "metadata": {},
   "source": [
    "### Step 5 - Init Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "de5735ee-5fe0-402d-99f5-3bd4344d363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # For reproducibility\n",
    "\n",
    "W1 = np.random.rand(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.rand(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "print(W1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecba2b-47fb-42f8-b962-a230ae262285",
   "metadata": {},
   "source": [
    "### Step 6 - Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "276a0636-5b03-4e59-9bb3-5e5a097c64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)  # Activation from the hidden layer\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)  # Final output activation\n",
    "    return A1, A2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00077762-2bd2-4d68-aa03-8e905bdbe191",
   "metadata": {},
   "source": [
    "### Step 7 - Compute The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a790da22-4ef2-47d3-84e0-757bcc90ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_old(Y, Y_hat):\n",
    "    m = Y.shape[0]\n",
    "    L = -(1./m) * (np.sum(np.multiply(Y, np.log(Y_hat)) + np.multiply(1-Y, np.log(1-Y_hat))))\n",
    "    return L\n",
    "\n",
    "def compute_loss(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Computes the categorical cross-entropy loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: actual labels (one-hot encoded), shape (m, num_classes)\n",
    "    - Y_hat: predicted probabilities, shape (m, num_classes), from softmax\n",
    "    \n",
    "    Returns:\n",
    "    - loss: the categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    loss = -np.sum(Y * np.log(Y_hat + 1e-9)) / m # Add a small value to prevent log(0)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdb71c-dee4-465d-a38f-d105e1d2e27f",
   "metadata": {},
   "source": [
    "### Step 8 - Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b1672030-c346-438c-8244-02e21e910a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_old(X, Y, W1, b1, W2, b2, A1, A2, learning_rate=0.01):\n",
    "    # Error at output layer\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Error at hidden layer\n",
    "    dZ1 = np.dot(dZ2, W2.T) * (1 - np.power(A1, 2))\n",
    "    dW1 = np.dot(X.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the ReLU function.\n",
    "    Parameters:\n",
    "    - Z: The input value(s).\n",
    "    \n",
    "    Returns:\n",
    "    - derivative: Gradient of Z where it is positive, and 0 otherwise.\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def backward_propagation(X, Y, W1, b1, W2, b2, A1, A2):\n",
    "    \"\"\"\n",
    "    Performs backward propagation and updates the parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input data.\n",
    "    - Y: True labels (one-hot encoded).\n",
    "    - W1, b1, W2, b2: Parameters of the network.\n",
    "    - A1, A2: Activations from forward propagation.\n",
    "    \n",
    "    Returns:\n",
    "    - dW1, db1, dW2, db2: Gradients of the loss with respect to W1, b1, W2, b2.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Gradient of loss with respect to Z2\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(A1.T, dZ2)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Backprop into the hidden layer\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(A1)\n",
    "    dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates the parameters using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    - W1, b1, W2, b2: Current parameters.\n",
    "    - dW1, db1, dW2, db2: Current gradients.\n",
    "    - learning_rate: Learning rate for the update.\n",
    "    \n",
    "    Returns:\n",
    "    - W1, b1, W2, b2: Updated parameters.\n",
    "    \"\"\"\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f0365-00f7-4a42-b73a-0009a78f0b7b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7e751499-02a8-4c6f-bb8c-41ff6567d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # For reproducibility\n",
    "\n",
    "W1 = np.random.rand(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.rand(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7a35cb36-14b9-45ad-97ce-e722450287a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.762518\n",
      "Cost after iteration 100: 0.749541\n",
      "Cost after iteration 200: 0.554209\n",
      "Cost after iteration 300: 0.466672\n",
      "Cost after iteration 400: 0.411088\n",
      "Cost after iteration 500: 0.366334\n",
      "Cost after iteration 600: 0.327500\n",
      "Cost after iteration 700: 0.293237\n",
      "Cost after iteration 800: 0.263159\n",
      "Cost after iteration 900: 0.237186\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate=0.01\n",
    "for i in range(1000): # Number of epochs\n",
    "    A1, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "    cost = compute_loss(y_train, A2)\n",
    "    #W1, b1, W2, b2 = backward_propagation(X_train, y_train, W1, b1, W2, b2, A1, A2, 0.001)\n",
    "    dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, W1, b1, W2, b2, A1, A2)\n",
    "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        print(\"Cost after iteration %i: %f\" % (i, cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b9577c53-a2d7-4a02-a9b2-293388fa9f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.14%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "_, A2_test = forward_propagation(X_test, W1, b1, W2, b2)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predictions = np.argmax(A2_test, axis=1)\n",
    "\n",
    "# Convert actual labels for comparison\n",
    "actual_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == actual_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "791f0285-8399-49cc-beb8-ff6fa61fd559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "149cefef-5d8a-41df-9b93-a6a7f3db0735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 1 2 0 2 1 1 1 0 1 2 0 1 1 0 0 2 2 0 0 0 2 2 2 0 1 0 0 1 0 1 1 2 2 1\n",
      " 2 1 1 1 2 1 1 0 1 1 1 1 1 1 1 1 2 0 2 2 0 1 2 1 0 0 1 2 2 0 0 0 0 0 1 2 1\n",
      " 2 2 2 2 0 1 0 0 2 0 0 1 0 1 2 0 0 2 0 2 2 2 1 2 2 0 2 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(actual_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6af2a-e064-49b9-8a6a-3556d7f77309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
