{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53276896-3a7c-4e32-a36f-69a5d22f0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43151154-c9f4-4485-a66f-9332d029ec93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "150\n",
      "[2 2 2 2 2 2 2 2 2 2]\n",
      "[[6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "5.1 3.5 1.4 0.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def export_network_params(W1, b1, W2, b2, filename=None):\n",
    "    network_params = {\n",
    "        \"W1\": W1.tolist(),\n",
    "        \"b1\": b1.tolist(),\n",
    "        \"W2\": W2.tolist(),\n",
    "        \"b2\": b2.tolist()\n",
    "    }\n",
    "    if filename:\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(network_params, file)\n",
    "        return \"\"\n",
    "    else:\n",
    "        network_params_json = json.dumps(network_params)\n",
    "        # Now network_params_json contains the JSON representation of network_params\n",
    "        return network_params_json\n",
    "\n",
    "\n",
    "print(type(iris.target))\n",
    "print(iris.target.size)\n",
    "print(iris.target[-10:])\n",
    "print(iris.data[-10:])\n",
    "print(\" \".join(map(str, iris.data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0163dd79-ef72-47d4-8f5d-f7b8756cc7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,) (150, 1)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "'''\n",
    "# since this is a bunch, create a dataframe\n",
    "iris_df=pd.DataFrame(iris.data)\n",
    "iris_df['class']=iris.target\n",
    "iris_df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "iris_df.dropna(how=\"all\", inplace=True) # remove any empty lines\n",
    "\n",
    "#view the iris dataframe\n",
    "print(iris_df)\n",
    "'''\n",
    "\n",
    "# Convert to DataFrame for convenience\n",
    "data = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = data.iloc[:, :-1].values\n",
    "y_val = y = data.iloc[:, -1].values\n",
    "y = y_val.reshape(-1, 1) # Reshape for one-hot encoding\n",
    "#print(type(y))\n",
    "print(y_val.shape, y.shape)\n",
    "print(y_val[:10])\n",
    "print(y[:10])\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "#print(X[:10])\n",
    "print(y[:10])\n",
    "#print(y[0].shape)\n",
    "#print(X[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226efbe-393b-4ff2-90e8-e3fef01a973e",
   "metadata": {},
   "source": [
    "### Step 3 - Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "825a2b46-8cb0-4a7e-91d0-dda002234456",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a36541-268d-47d8-8609-a40007a3fe8b",
   "metadata": {},
   "source": [
    "### Step 4 - Define Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1bac74c8-5021-481f-92e7-a6636da5fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1] # 4 features\n",
    "hidden_size = 1\n",
    "output_size = y_train.shape[1] # 3 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71ac53-6803-4996-8ccf-f658c691f7a0",
   "metadata": {},
   "source": [
    "### Step 5 - Init Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de5735ee-5fe0-402d-99f5-3bd4344d363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # For reproducibility\n",
    "\n",
    "W1 = np.random.rand(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.rand(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "print(W1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecba2b-47fb-42f8-b962-a230ae262285",
   "metadata": {},
   "source": [
    "### Step 6 - Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "276a0636-5b03-4e59-9bb3-5e5a097c64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)  # Activation from the hidden layer\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)  # Final output activation\n",
    "    return A1, A2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00077762-2bd2-4d68-aa03-8e905bdbe191",
   "metadata": {},
   "source": [
    "### Step 7 - Compute The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a790da22-4ef2-47d3-84e0-757bcc90ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Computes the categorical cross-entropy loss.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y: actual labels (one-hot encoded), shape (m, num_classes)\n",
    "    - Y_hat: predicted probabilities, shape (m, num_classes), from softmax\n",
    "    \n",
    "    Returns:\n",
    "    - loss: the categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    loss = -np.sum(Y * np.log(Y_hat + 1e-9)) / m # Add a small value to prevent log(0)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdb71c-dee4-465d-a38f-d105e1d2e27f",
   "metadata": {},
   "source": [
    "### Step 8 - Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b1672030-c346-438c-8244-02e21e910a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_old(X, Y, W1, b1, W2, b2, A1, A2, learning_rate=0.01):\n",
    "    # Error at output layer\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T, dZ2)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Error at hidden layer\n",
    "    dZ1 = np.dot(dZ2, W2.T) * (1 - np.power(A1, 2))\n",
    "    dW1 = np.dot(X.T, dZ1)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the ReLU function.\n",
    "    Parameters:\n",
    "    - Z: The input value(s).\n",
    "    \n",
    "    Returns:\n",
    "    - derivative: Gradient of Z where it is positive, and 0 otherwise.\n",
    "    \"\"\"\n",
    "    return (Z > 0).astype(int)\n",
    "\n",
    "def backward_propagation(X, Y, W1, b1, W2, b2, A1, A2):\n",
    "    \"\"\"\n",
    "    Performs backward propagation and updates the parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input data.\n",
    "    - Y: True labels (one-hot encoded).\n",
    "    - W1, b1, W2, b2: Parameters of the network.\n",
    "    - A1, A2: Activations from forward propagation.\n",
    "    \n",
    "    Returns:\n",
    "    - dW1, db1, dW2, db2: Gradients of the loss with respect to W1, b1, W2, b2.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Gradient of loss with respect to Z2\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(A1.T, dZ2)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Backprop into the hidden layer\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * relu_derivative(A1)\n",
    "    dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates the parameters using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    - W1, b1, W2, b2: Current parameters.\n",
    "    - dW1, db1, dW2, db2: Current gradients.\n",
    "    - learning_rate: Learning rate for the update.\n",
    "    \n",
    "    Returns:\n",
    "    - W1, b1, W2, b2: Updated parameters.\n",
    "    \"\"\"\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f0365-00f7-4a42-b73a-0009a78f0b7b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e751499-02a8-4c6f-bb8c-41ff6567d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(17) # For reproducibility\n",
    "\n",
    "W1 = np.random.rand(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.rand(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7a35cb36-14b9-45ad-97ce-e722450287a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.5752\n",
      "Cost after iteration 100: 1.0015\n",
      "Cost after iteration 200: 0.6260\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate=0.05\n",
    "num_epochs = 300\n",
    "for i in range(num_epochs): # Number of epochs\n",
    "    A1, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "    cost = compute_loss(y_train, A2)\n",
    "    dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, W1, b1, W2, b2, A1, A2)\n",
    "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Cost after iteration {i}: {cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cb9c30c0-c259-4a92-93c8-2d6167d8fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"W1\": [[0.036058761662048855], [-0.5856192722945736], [0.8556396852569221], [1.1326743771640153]], \"b1\": [[-0.24287472477262365]], \"W2\": [[-0.5613963146059882, 0.6609801416890401, 0.9577838116125987]], \"b2\": [[1.2081190713139576, -0.10388451577294054, -1.1042345555410178]]}\n"
     ]
    }
   ],
   "source": [
    "j = export_network_params(W1, b1, W2, b2)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b9577c53-a2d7-4a02-a9b2-293388fa9f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 1 2 0 2 1 1 1 0 2 2 0 2 2 0 0 2 2 0 0 0 2 2 2 0 1 0 0]\n",
      "Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "_, A2_test = forward_propagation(X_test, W1, b1, W2, b2)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predictions = np.argmax(A2_test, axis=1)\n",
    "print(f\"{predictions}\")\n",
    "\n",
    "\n",
    "# Convert actual labels for comparison\n",
    "actual_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == actual_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "791f0285-8399-49cc-beb8-ff6fa61fd559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 2 2 2 2 1 2 1 2 1 2 0 2 2 1 2 2 2 1 2 2 0 2 2 0 2 2 1 0 0 2 2 2 0\n",
      " 0 0 0 0 1 2 2 2 2 2 2 0 1 0 0 2 0 0 2 0 2 2 0 0 2 0 2 2 2 1 2 2 0 2 2 2 2\n",
      " 0 0 0 2 0 2 0 2 2 2 0 2 2 0 2 0 2 2 0 2 2 2 1 2 0 2 0 2 2 0 2 2 2 2 0 2 2\n",
      " 2 2 2 0 2 0 0 1 2]\n",
      "Accuracy: 76.67%\n"
     ]
    }
   ],
   "source": [
    "_, A2_train = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predictions = np.argmax(A2_train, axis=1)\n",
    "print(f\"{predictions}\")\n",
    "\n",
    "\n",
    "# Convert actual labels for comparison\n",
    "actual_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == actual_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "149cefef-5d8a-41df-9b93-a6a7f3db0735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 2 2 1 2 1 1 1 2 1 1 0 1 1 1 1 1 1 1 1 2 0 2 2 0 1 2 1 0 0 1 2 2 0\n",
      " 0 0 0 0 1 2 1 2 2 2 2 0 1 0 0 2 0 0 1 0 1 2 0 0 2 0 2 2 2 1 2 2 0 2 2 1 2\n",
      " 0 0 0 2 0 2 0 2 1 2 0 1 1 0 2 0 2 1 0 2 1 2 1 1 0 2 0 2 1 0 2 1 2 2 0 2 1\n",
      " 2 1 1 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(actual_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15b6af2a-e064-49b9-8a6a-3556d7f77309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.8, 3.1, 1.6, 0.2]), array([1., 0., 0.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba603719-4056-4695-b365-ac5af9a99092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
