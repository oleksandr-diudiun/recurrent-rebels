{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4634f49e-4859-4015-b368-990d6ce11de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_len  sepal_wid  petal_len  petal_wid  class\n",
      "0          5.1        3.5        1.4        0.2      0\n",
      "1          4.9        3.0        1.4        0.2      0\n",
      "2          4.7        3.2        1.3        0.2      0\n",
      "3          4.6        3.1        1.5        0.2      0\n",
      "4          5.0        3.6        1.4        0.2      0\n",
      "..         ...        ...        ...        ...    ...\n",
      "145        6.7        3.0        5.2        2.3      2\n",
      "146        6.3        2.5        5.0        1.9      2\n",
      "147        6.5        3.0        5.2        2.0      2\n",
      "148        6.2        3.4        5.4        2.3      2\n",
      "149        5.9        3.0        5.1        1.8      2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# since this is a bunch, create a dataframe\n",
    "iris_df=pd.DataFrame(iris.data)\n",
    "iris_df['class']=iris.target\n",
    "iris_df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "iris_df.dropna(how=\"all\", inplace=True) # remove any empty lines\n",
    "\n",
    "print(iris_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3375a558-169b-413b-ada7-521c9694e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayerSize = 4\n",
    "hiddenLayerSize = 8\n",
    "outputLayerSize = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdd10210-59b3-41ea-87d0-ad06269ac56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8359929  0.04377293 0.93373356 0.84296813]\n",
      " [0.41337967 0.95927489 0.95516153 0.04555595]\n",
      " [0.30152866 0.32845921 0.59567272 0.53964447]\n",
      " [0.30679836 0.36642388 0.79843557 0.44404882]\n",
      " [0.47404939 0.86291363 0.10742296 0.8106241 ]\n",
      " [0.46298523 0.80491773 0.34365313 0.50692951]\n",
      " [0.69682609 0.85283356 0.07915453 0.19745055]\n",
      " [0.72504537 0.96279694 0.9830814  0.7943539 ]]\n",
      " \n",
      "[[0.44627374 0.93686845 0.9728287  0.13432685 0.05895633 0.38345359\n",
      "  0.5207814  0.51166993]\n",
      " [0.45790776 0.89029982 0.89814957 0.75024143 0.16371903 0.37928741\n",
      "  0.84385727 0.34901862]\n",
      " [0.28621917 0.60669242 0.98921577 0.06115837 0.67903111 0.04856148\n",
      "  0.83354633 0.62637431]]\n"
     ]
    }
   ],
   "source": [
    "# Random Weight\n",
    "\n",
    "# First Random Weight matrix Input to Hidden layers\n",
    "W1 = np.random.rand(hiddenLayerSize, inputLayerSize)\n",
    "# First Random Weight matrix Hidden to Output layers \n",
    "W2 = np.random.rand(outputLayerSize, hiddenLayerSize)\n",
    "print(W1)\n",
    "print(\" \")\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64fdcb99-7d84-433a-8d63-7b7d5e88ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      " \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Starting Bias for hidden layer\n",
    "b1 = np.zeros((hiddenLayerSize, 1))\n",
    "# Starting Bias for hidden layer\n",
    "b2 = np.zeros((outputLayerSize, 1))\n",
    "print(b1)\n",
    "print(\" \")\n",
    "print(b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa1ea4-8bc4-4817-853e-d1cd98f43f1d",
   "metadata": {},
   "source": [
    "    Implements the ReLU activation function.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4522d357-ce28-4c06-853c-3fcc902b36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    " def ActivationFunction_relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = {\"A\": A}\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb7bca-23e8-4b89-8bc9-48a9ae5c7d3d",
   "metadata": {},
   "source": [
    "Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter\n",
    "    cache -- a python dictionary containing \"A_prev\", \"W\" and \"b\" ; stored for computing the backward pass efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e69cfda-6a79-452e-bc17-1270cf69153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    Z = W.dot(A_prev) + b\n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = {\"A_prev\": A_prev, \"W\": W, \"b\": b}\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02e0e8-9122-4abc-8af5-195662e6dd61",
   "metadata": {},
   "source": [
    "    Implements the ReLU activation function.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a804ad30-29c3-499d-add3-4e09d12cb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = {\"A\": A}\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f2b88-7c90-4e94-a92b-d45e3a5aa0a8",
   "metadata": {},
   "source": [
    " \n",
    "    Implement forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value\n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1788920c-40e3-48d0-94d2-a05b5ac99677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(A_prev, W, b):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ba0c0cc-724b-4c8b-8558-a33601bcfb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input\n",
    "X = np.random.randn(4, 1) # 4 features, 1 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "baa09b94-5bc5-46bc-9aa4-dd7bea74feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation from input to hidden layer\n",
    "A1, cache1 = forward_propagation(X, W1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63404a18-ef69-4d6a-b329-449412fa63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation from hidden layer to output\n",
    "A2, cache2 = forward_propagation(A1, W2, b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c96c1bd7-5656-47ce-b451-80316d88545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the network: [[0.17583526]\n",
      " [0.18041915]\n",
      " [0.11277254]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output of the network:\", A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb098b5-fbe1-46da-837c-64663ffcc565",
   "metadata": {},
   "source": [
    " \n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd7222-efcd-4289-8c19-c479f9e011ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "    dZ[Z <= 0] = 0  # When z <= 0, you should set dz to 0 as well.\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ef5b3-c1c7-4850-a910-55bd6958e33b",
   "metadata": {},
   "source": [
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    dB -- Gradient of the cost with respect to b (current layer l), same shape as b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82967143-06b4-4228-9aff-bce9aa06bf37",
   "metadata": {},
   "source": [
    "### Linear backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853647c-82bc-43a9-b0bf-1a12ac5385f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache['A_prev'], cache['W'], cache['b']\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92d8c0-6f01-45c7-be65-84e426424d6f",
   "metadata": {},
   "source": [
    "Implement the backward propagation for LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    dA2 -- post-activation gradient for output\n",
    "    cache2 -- cache stored during forward propagation for the second LINEAR->RELU step\n",
    "    cache1 -- cache stored during forward propagation for the first LINEAR->RELU step\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients dA1, dW2, db2, dW1, db1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da683525-b431-422e-91a3-a3e3d172cf62",
   "metadata": {},
   "source": [
    "### Compute the Categorical Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0daec4f-d368-42d4-86da-a8002ea8aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(Y, Y_hat):\n",
    "    m = Y.shape[1]\n",
    "    loss = -np.sum(Y * np.log(Y_hat)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa4ea7-459a-446e-9ca6-41c27f375f75",
   "metadata": {},
   "source": [
    "    Implement the backward propagation for the last layer using softmax and Categorical Cross-Entropy\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- Gradient of the loss with respect to the output of the last linear layer (Z)\n",
    "    cache2 -- cache stored during forward propagation for the second LINEAR->RELU step\n",
    "    cache1 -- cache stored during forward propagation for the first LINEAR->RELU step\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients dA1, dW2, db2, dW1, db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5327a-e084-4114-a717-fe8a115c7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_with_softmax(dZ, cache2, cache1):\n",
    "    # Assuming cache2 and cache1 include the necessary caches for the respective layers\n",
    "    dA1, dW2, db2 = linear_backward(dZ, cache2[\"linear\"])\n",
    "    \n",
    "    linear_cache1, activation_cache1 = cache1\n",
    "    dZ1 = relu_backward(dA1, activation_cache1['A'])\n",
    "    dA0, dW1, db1 = linear_backward(dZ1, linear_cache1)\n",
    "    \n",
    "    grads = {\"dA1\": dA1, \"dW2\": dW2, \"db2\": db2, \"dW1\": dW1, \"db1\": db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a300c-4877-47a3-8f29-a6d52fc39349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(dA2, cache2, cache1):\n",
    "    linear_cache2, activation_cache2 = cache2\n",
    "    dZ2 = relu_backward(dA2, activation_cache2['A'])\n",
    "    dA1, dW2, db2 = linear_backward(dZ2, linear_cache2)\n",
    "    \n",
    "    linear_cache1, activation_cache1 = cache1\n",
    "    dZ1 = relu_backward(dA1, activation_cache1['A'])\n",
    "    dA0, dW1, db1 = linear_backward(dZ1, linear_cache1)\n",
    "    \n",
    "    grads = {\"dA1\": dA1, \"dW2\": dW2, \"db2\": db2, \"dW1\": dW1, \"db1\": db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918bd49-b57b-4c30-ba6d-29d8b09ecb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    parameters[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
    "    parameters[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
    "    parameters[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
    "    parameters[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecee1e5-3462-443d-8566-ff1a912092ab",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680e4de-2cd6-45c1-b9d7-02f13b041dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, n_h, num_iterations=1000, learning_rate=0.01):\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        A1, cache1 = forward_propagation(X, W1, b1)\n",
    "        A2, cache2 = softmax(np.dot(W2, A1) + b2)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(Y, A2)\n",
    "        \n",
    "        # Backward propagation\n",
    "        dZ2 = A2 - Y\n",
    "        grads = backward_propagation_with_softmax(dZ2, cache2, cache1)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Loss after iteration %i: %f\" %(i, loss))\n",
    "    \n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
