{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d2f046-68d6-4589-8d9c-bf6b9be93f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# load iris dataset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a02858-1a30-4677-a5fc-46dd4bd5e9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "     sepal_len  sepal_wid  petal_len  petal_wid  class\n",
      "0          4.6        3.2        1.4        0.2      0\n",
      "1          6.7        3.1        5.6        2.4      2\n",
      "2          5.1        3.3        1.7        0.5      0\n",
      "3          5.7        4.4        1.5        0.4      0\n",
      "4          6.7        3.3        5.7        2.5      2\n",
      "..         ...        ...        ...        ...    ...\n",
      "145        5.0        2.3        3.3        1.0      1\n",
      "146        7.7        3.8        6.7        2.2      2\n",
      "147        6.5        3.2        5.1        2.0      2\n",
      "148        6.5        3.0        5.2        2.0      2\n",
      "149        5.6        3.0        4.1        1.3      1\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# since this is a bunch, create a dataframe\n",
    "iris_df=pd.DataFrame(iris.data)\n",
    "iris_df['class']=iris.target\n",
    "iris_df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "iris_df.dropna(how=\"all\", inplace=True) # remove any empty lines\n",
    "\n",
    "#view the iris dataframe\n",
    "print(type(iris_df))\n",
    "\n",
    "shuffled_df = iris_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(shuffled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c04898-4029-4513-8ea7-1672a2bd89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define our neural network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Hidden layer initialization\n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.rand(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Output layer initialization\n",
    "        self.W2 = np.random.rand(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        # Subtract the max value from each score to prevent overflow in exp\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculate the activations in the hidden layer\n",
    "        self.Z1 = np.dot(x, self.W1) + self.b1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "\n",
    "        # calculate activations for the output layer\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        y_hat = self.softmax(self.Z2)\n",
    "        return y_hat\n",
    "\n",
    "    def backprop(self, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        # Forward pass, Returns A2\n",
    "        predictions = self.forward(X)\n",
    "        \n",
    "        # Calculate the error of the output layer vs ground truth\n",
    "        loss = predictions - y\n",
    "        # Calculate the dW2, i.e. the derivatives of the Loss function wiht regards to the Loss function\n",
    "        dW2 = (1/m) * np.dot(self.A1.T, loss)\n",
    "\n",
    "        # Calculate the \n",
    "        dZ1 = np.dot(loss, self.W2.T) * (self.Z1 > 0)\n",
    "        dW1 = (1/m) * np.dot(X.T, dZ1)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * (1/m) * np.sum(loss, axis=0, keepdims=True)\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, predictions, targets):\n",
    "        return -np.mean(targets * np.log(predictions + 1e-9))\n",
    "\n",
    "    def train(self, X_train, Y_train, X_test, Y_test, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.forward(X_train)\n",
    "            loss = self.compute_loss(predictions, Y_train)\n",
    "\n",
    "            self.backprop(X_train, Y_train, learning_rate)\n",
    "            #print(f\"W2: {self.W2}\")\n",
    "\n",
    "            # Compute accuracy on testing data\n",
    "            test_predictions = np.argmax(self.forward(X_test), axis=1)\n",
    "            test_accuracy = np.mean(test_predictions == np.argmax(Y_test, axis=1))\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {test_accuracy*100:.4f}%\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e3dcd17-2189-4ff9-bf05-db92cd39b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_hot_encoder(labels):\n",
    "    return np.unique(labels)\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]   \n",
    "# def transform_one_hot_encoder(labels, categories):\n",
    "#     # Create a dictionary to map each category to a unique index\n",
    "#     category_index = {category: idx for idx, category in enumerate(categories)}\n",
    "#     print(type(category_index))\n",
    "    \n",
    "#     # Initialize the one-hot encoded matrix with zeros\n",
    "#     one_hot_encoded = np.zeros((len(labels), len(categories)))\n",
    "    \n",
    "#     # Encode each label as a one-hot vector\n",
    "#     for i, label in enumerate(labels):\n",
    "#         scalar_label = label[0]\n",
    "#         index = category_index[scalar_label]\n",
    "#         one_hot_encoded[i, index] = 1\n",
    "        \n",
    "#     return one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b167be0c-389b-4976-8d8e-66eb6b8e56a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4015, Test Accuracy: 46.6667%\n",
      "Epoch 100, Loss: 0.1644, Test Accuracy: 76.6667%\n",
      "Epoch 200, Loss: 0.1237, Test Accuracy: 90.0000%\n",
      "Epoch 300, Loss: 0.0983, Test Accuracy: 96.6667%\n",
      "Epoch 400, Loss: 0.0805, Test Accuracy: 93.3333%\n",
      "Epoch 500, Loss: 0.0686, Test Accuracy: 93.3333%\n",
      "Epoch 600, Loss: 0.0604, Test Accuracy: 93.3333%\n",
      "Epoch 700, Loss: 0.0545, Test Accuracy: 93.3333%\n",
      "Epoch 800, Loss: 0.0499, Test Accuracy: 93.3333%\n",
      "Epoch 900, Loss: 0.0464, Test Accuracy: 93.3333%\n",
      "[[0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]]\n",
      "[[0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]]\n",
      "test_accuracy: 0.9733333333333334\n",
      "index: 42, feature: [6.  2.7 5.1 1.6], Y: [1.39381051e-08 2.49410066e-01 7.50589920e-01]\n",
      "index: 90, feature: [6.3 2.5 4.9 1.5], Y: [5.82086427e-07 4.89168206e-01 5.10831211e-01]\n",
      "index: 121, feature: [5.9 3.2 4.8 1.8], Y: [2.11177867e-07 4.16214829e-01 5.83784960e-01]\n",
      "index: 136, feature: [5.1 2.5 3.  1.1], Y: [0.62240157 0.37467504 0.00292339]\n"
     ]
    }
   ],
   "source": [
    "# create a new instance of a neural network\n",
    "nn = NeuralNetwork(input_size = 4, hidden_size = 1, output_size = 3)\n",
    "\n",
    "X = shuffled_df[['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid']].values\n",
    "\n",
    "X_train, X_val = np.split(X, [120])\n",
    "\n",
    "y = shuffled_df['class'].values\n",
    "\n",
    "#y_onehot = transform_one_hot_encoder(y, fit_one_hot_encoder(y))\n",
    "y_onehot = one_hot_encode(y, 3)\n",
    "Y_train_one_hot, Y_val_one_hot = np.split(y_onehot, [120])\n",
    "\n",
    "# ----------------------\n",
    "# Paul test\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Y = shuffled_df['class'].values.reshape(-1, 1)\n",
    "\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# encoder = OneHotEncoder()\n",
    "# Y_train_one_hot = encoder.fit_transform(Y_train).toarray()  # Fit and transform Y_train\n",
    "# Y_val_one_hot = encoder.transform(Y_val).toarray()  # Transform Y_val based on the same encoding\n",
    "\n",
    "#-----------------------\n",
    "# Try with new order\n",
    "# X_train, X_test = np.split(X, [120])\n",
    "\n",
    "# y = shuffled_df['class'].values\n",
    "\n",
    "# y_train, y_test = np.split(y, [120])\n",
    "\n",
    "# Y_train_one_hot = one_hot_encode(y_train, 3)\n",
    "# Y_val_one_hot = one_hot_encode(y_test, 3)\n",
    "\n",
    "# Print our values\n",
    "# print(f\"X_train: {X_train}\")\n",
    "# print(f\"Y_train_one_hot: {Y_train_one_hot}\")\n",
    "# print(f\"X_val: {X_val}\")\n",
    "# print(f\"Y_val_one_hot: {Y_val_one_hot}\")\n",
    "\n",
    "nn.train(X_train, Y_train_one_hot, X_val, Y_val_one_hot, epochs=1000, learning_rate=0.125)\n",
    "\n",
    "result = nn.forward(X)\n",
    "test_predictions = np.argmax(nn.forward(X), axis=1).reshape(-1, 1)\n",
    "print(test_predictions)\n",
    "print(Y)\n",
    "test_accuracy = np.mean(test_predictions == Y)\n",
    "\n",
    "print(f\"test_accuracy: {test_accuracy}\")\n",
    "\n",
    "for index in range(test_predictions.size):\n",
    "    if test_predictions[index] != Y[index]:\n",
    "        print(f\"index: {index}, feature: {X[index]}, Y: {result[index]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df956dec-f3ce-4bf4-acfb-3e6782ea9273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203858a-41c2-45ed-99ee-77beba45405b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
